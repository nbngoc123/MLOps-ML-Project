import mlflow
import os
import logging
import joblib
import s3fs
import asyncio
import pandas as pd
import shutil
import numpy as np
from io import BytesIO
from fastapi import HTTPException, UploadFile, File
from pydantic import BaseModel
from mlflow.tracking import MlflowClient

# ========================
# Config
# ========================
logger = logging.getLogger("trend-controller")
logging.basicConfig(level=logging.INFO)

# 1. Config Environment
MLFLOW_TRACKING_URI = os.getenv("MLFLOW_TRACKING_URI", "http://mlflow:5000")
MINIO_ENDPOINT = os.getenv("MINIO_ENDPOINT", "http://minio:9000")
MINIO_ACCESS_KEY = os.getenv("MINIO_ACCESS_KEY", "minioadmin")
MINIO_SECRET_KEY = os.getenv("MINIO_SECRET_KEY", "minioadmin")
MINIO_BUCKET = "nexusml"

# T√™n Model kh·ªõp v·ªõi Config trong DAG
SENTIMENT_MODEL_NAME = "sentiment_classifier_Model"
TOPIC_MODEL_NAME = "Nexus_Topic_Classifier"
ALIAS = "production"

# Global variables
sentiment_model = None
topic_model = None
are_models_ready = False

# ========================
# 1. H√†m Load Multiple Models
# ========================
async def load_trend_models(retries=3, delay=2):
    """
    Load ƒë·ªìng th·ªùi c·∫£ Sentiment Model v√† Topic Model
    """
    global sentiment_model, topic_model, are_models_ready
    
    client = MlflowClient(tracking_uri=MLFLOW_TRACKING_URI)
    
    for attempt in range(1, retries+1):
        try:
            logger.info(f"üîÑ [Attempt {attempt}] Loading Trend Analysis Models...")

            # --- A. Load Sentiment Model ---
            mv_sent = client.get_model_version_by_alias(SENTIMENT_MODEL_NAME, ALIAS)
            path_sent = mlflow.artifacts.download_artifacts(run_id=mv_sent.run_id)
            
            # T√¨m file model.pkl cho Sentiment
            sent_file = None
            for root, _, files in os.walk(path_sent):
                if "model.pkl" in files:
                    sent_file = os.path.join(root, "model.pkl")
                    break
            if not sent_file: raise FileNotFoundError("Sentiment model.pkl not found")
            sentiment_model = joblib.load(sent_file)
            logger.info("‚úÖ Loaded Sentiment Model")

            # --- B. Load Topic Model ---
            mv_topic = client.get_model_version_by_alias(TOPIC_MODEL_NAME, ALIAS)
            path_topic = mlflow.artifacts.download_artifacts(run_id=mv_topic.run_id)
            
            # T√¨m file model.pkl cho Topic
            topic_file = None
            for root, _, files in os.walk(path_topic):
                if "model.pkl" in files:
                    topic_file = os.path.join(root, "model.pkl")
                    break
            if not topic_file: raise FileNotFoundError("Topic model.pkl not found")
            topic_model = joblib.load(topic_file)
            logger.info("‚úÖ Loaded Topic Model")

            # --- Cleanup & Finish ---
            are_models_ready = True
            try:
                shutil.rmtree(path_sent)
                shutil.rmtree(path_topic)
            except: pass
            
            return

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Load failed ({e}). Retrying in {delay}s...")
            are_models_ready = False
            if attempt < retries:
                await asyncio.sleep(delay)
            else:
                logger.error("‚ùå Final failure loading Trend models.")

# ========================
# 2. API Endpoints
# ========================

async def startup_event():
    await load_trend_models()

async def analyze_trend_csv(file: UploadFile = File(...)):
    """
    Input: File CSV ch·ª©a c·ªôt 'date' v√† 'comment' (ho·∫∑c 'text').
    Output: JSON (d·∫°ng b·∫£ng CSV) ch·ª©a th·ªëng k√™ xu h∆∞·ªõng theo ng√†y.
    """
    if not are_models_ready:
        await load_trend_models()
        if not are_models_ready: raise HTTPException(503, "Models not ready")

    try:
        # 1. ƒê·ªçc File
        content = await file.read()
        df = pd.read_csv(BytesIO(content))
        
        # Validate Columns
        text_col = next((col for col in ['comment', 'text', 'content', 'feedback'] if col in df.columns), None)
        date_col = next((col for col in ['date', 'time', 'created_at'] if col in df.columns), None)
        
        if not text_col or not date_col:
            raise HTTPException(400, "CSV ph·∫£i ch·ª©a c·ªôt th·ªùi gian ('date') v√† n·ªôi dung ('comment'/'text')")

        logger.info(f"Analyzing trends for {len(df)} records...")

        # 2. Preprocessing (Gi·ªëng logic trong DAG)
        # X·ª≠ l√Ω Date
        df['date'] = pd.to_datetime(df[date_col], utc=True, errors='coerce')
        df = df.dropna(subset=['date'])
        
        # X·ª≠ l√Ω Text
        clean_text = df[text_col].astype(str).str.lower().fillna("")
        clean_text = clean_text.str.replace(r'[^\w\s]', '', regex=True) # B·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát
        
        # 3. Inference (Ch·∫°y 2 model)
        # Predict Topic
        df['topic'] = topic_model.predict(clean_text)
        
        # Predict Sentiment & Map Labels
        sent_preds = sentiment_model.predict(clean_text)
        
        # Map k·∫øt qu·∫£ Sentiment (Gi·∫£ s·ª≠ model tr·∫£ v·ªÅ 0,1,2 ho·∫∑c chu·ªói)
        # Logic mapping cho nh·∫•t qu√°n v·ªõi b√°o c√°o
        def map_sentiment(val):
            s_map = {0: 'Negative', 1: 'Positive', 2: 'Neutral'}
            if isinstance(val, (int, np.integer)):
                return s_map.get(val, 'Neutral')
            return str(val).capitalize() # N·∫øu model ƒë√£ tr·∫£ v·ªÅ chu·ªói 'spam'/'ham' etc.

        df['sentiment'] = [map_sentiment(x) for x in sent_preds]

        # 4. Aggregation (T·∫°o b·∫£ng b√°o c√°o)
        # Group theo Ng√†y (Daily) + Topic + Sentiment
        trend_df = df.groupby([
            pd.Grouper(key='date', freq='D'), 
            'topic', 
            'sentiment'
        ]).size().reset_index(name='volume')

        # Pivot Table: Bi·∫øn Sentiment th√†nh c√°c c·ªôt (Negative, Positive, Neutral)
        pivot_trend = trend_df.pivot_table(
            index=['date', 'topic'], 
            columns='sentiment', 
            values='volume', 
            fill_value=0
        ).reset_index()

        # ƒê·∫£m b·∫£o c√≥ ƒë·ªß c·ªôt ƒë·ªÉ kh√¥ng l·ªói frontend
        for col in ['Negative', 'Positive', 'Neutral']:
            if col not in pivot_trend.columns:
                pivot_trend[col] = 0

        # T√≠nh ch·ªâ s·ªë ph·ª•
        pivot_trend['total_volume'] = pivot_trend['Negative'] + pivot_trend['Positive'] + pivot_trend['Neutral']
        pivot_trend['negative_rate'] = pivot_trend.apply(
            lambda x: round(x['Negative'] / x['total_volume'], 2) if x['total_volume'] > 0 else 0.0, 
            axis=1
        )

        # Format Date th√†nh string cho JSON
        pivot_trend['date'] = pivot_trend['date'].dt.strftime('%Y-%m-%d')

        # 5. Return Result
        # Tr·∫£ v·ªÅ d·∫°ng list of dicts, Frontend c√≥ th·ªÉ hi·ªÉn th·ªã table ho·∫∑c cho user t·∫£i v·ªÅ CSV
        result_data = pivot_trend.to_dict(orient="records")

        return {
            "filename": file.filename,
            "total_days": len(pivot_trend),
            "data": result_data,
            "columns": list(pivot_trend.columns) # G·ª≠i k√®m t√™n c·ªôt ƒë·ªÉ frontend d·ªÖ render header
        }

    except Exception as e:
        logger.error(f"Trend Analysis Error: {e}")
        raise HTTPException(500, f"L·ªói ph√¢n t√≠ch xu h∆∞·ªõng: {str(e)}")