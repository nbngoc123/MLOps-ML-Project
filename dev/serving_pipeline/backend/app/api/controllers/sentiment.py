import mlflow
import os
import logging
import joblib
import s3fs
import asyncio
import pandas as pd
import shutil
from io import BytesIO
from fastapi import HTTPException, UploadFile, File
from pydantic import BaseModel
from mlflow.tracking import MlflowClient
# ========================
# Config
# ========================
logger = logging.getLogger("sentiment-controller")
logging.basicConfig(level=logging.INFO)

# 1. Config cho MLflow (Ch·ªâ ƒë·ªÉ l·∫•y Run ID)
MLFLOW_TRACKING_URI = os.getenv("MLFLOW_TRACKING_URI", "http://mlflow:5000")

# 2. Config cho S3FS (ƒê·ªÉ t·∫£i file tr·ª±c ti·∫øp)
MINIO_ENDPOINT = os.getenv("MINIO_ENDPOINT", "http://minio:9000")
MINIO_ACCESS_KEY = os.getenv("MINIO_ACCESS_KEY", "minioadmin")
MINIO_SECRET_KEY = os.getenv("MINIO_SECRET_KEY", "minioadmin")
MINIO_BUCKET = "nexusml"

MODEL_NAME = "SentimentClassifier"
ALIAS = "production"

# Kh·ªüi t·∫°o k·∫øt n·ªëi S3
fs = s3fs.S3FileSystem(
    client_kwargs={'endpoint_url': MINIO_ENDPOINT},
    key=MINIO_ACCESS_KEY,
    secret=MINIO_SECRET_KEY
)

vectorizer = None
model = None
is_model_ready = False

class SentimentInput(BaseModel):
    text: str

# ========================
# 1. H√†m Load Model (Robust Version)
# ========================
async def load_sentiment_model(retries=3, delay=2):
    global vectorizer, model, is_model_ready
    
    client = MlflowClient(tracking_uri=MLFLOW_TRACKING_URI)
    
    for attempt in range(1, retries+1):
        try:
            logger.info(f"üîÑ [Attempt {attempt}] Finding production model for '{MODEL_NAME}'...")
            
            # 1. L·∫•y Run ID t·ª´ Alias
            mv = client.get_model_version_by_alias(MODEL_NAME, ALIAS)
            run_id = mv.run_id
            model_source = mv.source # e.g., s3://nexusml/models/.../artifacts/model
            logger.info(f"üéØ Found Production Run ID: {run_id}")

            # 2. Download Artifacts v·ªÅ th∆∞ m·ª•c t·∫°m
            # L√Ω do: D√πng mlflow download an to√†n h∆°n t·ª± m√≤ ƒë∆∞·ªùng d·∫´n S3 khi c·∫•u tr√∫c folder thay ƒë·ªïi
            local_path = mlflow.artifacts.download_artifacts(run_id=run_id)
            
            # 3. T√¨m file model.pkl v√† vectorizer.pkl (ƒê·ªá quy)
            model_file = None
            vec_file = None
            
            for root, dirs, files in os.walk(local_path):
                if "model.pkl" in files:
                    model_file = os.path.join(root, "model.pkl")
                if "vectorizer.pkl" in files:
                    vec_file = os.path.join(root, "vectorizer.pkl")

            if not model_file or not vec_file:
                raise FileNotFoundError("‚ùå Kh√¥ng t√¨m th·∫•y model.pkl ho·∫∑c vectorizer.pkl trong artifacts t·∫£i v·ªÅ!")

            # 4. Load v√†o RAM
            model = joblib.load(model_file)
            vectorizer = joblib.load(vec_file)
            is_model_ready = True
            
            logger.info(f"‚úÖ Model Loaded Successfully from {local_path}")
            
            # D·ªçn d·∫πp folder t·∫°m ƒë·ªÉ ti·∫øt ki·ªám dung l∆∞·ª£ng
            try: shutil.rmtree(local_path) 
            except: pass
            return

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Load failed ({e}). Retrying in {delay}s...")
            is_model_ready = False
            if attempt < retries:
                await asyncio.sleep(delay)
            else:
                logger.error("‚ùå Final failure loading model.")

# ========================
# 2. API Endpoints
# ========================

async def startup_event():
    await load_sentiment_model()

async def predict_single(data: SentimentInput):
    """D·ª± ƒëo√°n 1 c√¢u text duy nh·∫•t"""
    if not is_model_ready:
        await load_sentiment_model()
        if not is_model_ready: raise HTTPException(503, "Model not ready")

    try:
        # Transform & Predict
        vec = vectorizer.transform([data.text])
        pred = model.predict(vec)[0]
        
        # Map label
        label_map = {0: "ti√™u c·ª±c", 1: "t√≠ch c·ª±c", 2: "trung t√≠nh"}
        sentiment = label_map.get(int(pred), "unknown")
        
        # L·∫•y confidence score n·∫øu c√≥
        confidence = 0.0
        if hasattr(model, "predict_proba"):
            confidence = float(model.predict_proba(vec).max())

        return {
            "text": data.text,
            "sentiment": sentiment,
            "confidence": round(confidence, 4),
            "run_source": "mlflow_production"
        }
    except Exception as e:
        logger.error(f"Error: {e}")
        raise HTTPException(500, str(e))

async def predict_batch(file: UploadFile = File(...)):
    """
    Upload file CSV -> Tr·∫£ v·ªÅ JSON ch·ª©a d·ªØ li·ªáu ƒë√£ ph√¢n lo·∫°i.
    D√†nh cho Frontend hi·ªÉn th·ªã b·∫£ng k·∫øt qu·∫£ ngay l·∫≠p t·ª©c.
    """
    if not is_model_ready:
        await load_sentiment_model()
        if not is_model_ready: raise HTTPException(503, "Model not ready")

    try:
        # 1. ƒê·ªçc file CSV t·ª´ Upload
        content = await file.read()
        df = pd.read_csv(BytesIO(content))
        
        # Ki·ªÉm tra c·ªôt
        text_col = next((col for col in ['text', 'comment', 'content'] if col in df.columns), None)
        if not text_col:
            raise HTTPException(400, "CSV ph·∫£i ch·ª©a c·ªôt 'text', 'comment' ho·∫∑c 'content'")

        logger.info(f"Processing batch of {len(df)} records...")

        # 2. Batch Processing (Vectorization)
        # X·ª≠ l√Ω h√†ng lo·∫°t c·ª±c nhanh, kh√¥ng d√πng loop
        raw_text = df[text_col].astype(str).str.lower().fillna("")
        X_vec = vectorizer.transform(raw_text)

        # 3. Predict
        predictions = model.predict(X_vec)
        
        # 4. Map Label & Confidence
        label_map = {0: "ti√™u c·ª±c", 1: "t√≠ch c·ª±c", 2: "trung t√≠nh"}
        
        # G√°n k·∫øt qu·∫£ v√†o DataFrame
        df['prediction_code'] = predictions
        if pd.api.types.is_numeric_dtype(df['prediction_code']):
             df['sentiment'] = df['prediction_code'].map(label_map)
        else:
             df['sentiment'] = df['prediction_code'] # Tr∆∞·ªùng h·ª£p model tr·∫£ v·ªÅ string s·∫µn

        # T√≠nh confidence
        if hasattr(model, "predict_proba"):
            probs = model.predict_proba(X_vec).max(axis=1)
            df['confidence'] = probs.round(4)

        # 5. Chuy·ªÉn ƒë·ªïi DF th√†nh List of Dicts ƒë·ªÉ tr·∫£ v·ªÅ JSON
        # orient='records' t·∫°o ra c·∫•u tr√∫c: [{"text": "...", "sentiment": "..."}, ...]
        result_data = df.to_dict(orient="records")

        return {
            "filename": file.filename,
            "total_rows": len(df),
            "data": result_data,  # <--- TR·∫¢ V·ªÄ DATA ƒê√É GH√âP B·∫¢NG
            "run_source": "mlflow_production_batch"
        }

    except Exception as e:
        logger.error(f"Batch error: {e}")
        raise HTTPException(500, f"L·ªói x·ª≠ l√Ω file: {str(e)}")