import gradio as gr
import os
import pandas as pd
import numpy as np
import httpx
import asyncio
import time 
import matplotlib.pyplot as plt
import seaborn as sns
from dotenv import load_dotenv

import matplotlib
matplotlib.use('Agg')

load_dotenv()
BACKEND_URL = os.getenv("BACKEND_URL", "http://localhost:8000")

# Thi·∫øt l·∫≠p th∆∞ m·ª•c t·∫°m cho Gradio
GRADIO_TEMP_DIR = "/tmp/gradio"
os.environ["GRADIO_TEMP_DIR"] = GRADIO_TEMP_DIR
os.makedirs(GRADIO_TEMP_DIR, exist_ok=True) # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥

# ========================
# Utils: Call API
# ========================

async def call_api_post(endpoint, json_data):
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            res = await client.post(f"{BACKEND_URL}{endpoint}", json=json_data)
            res.raise_for_status()
            return res.json()
    except Exception as e:
        return {"error": str(e)}


async def send_csv_batch(endpoint, file_obj):
    if file_obj is None:
        return pd.DataFrame({"L·ªói": ["Ch∆∞a ch·ªçn file"]})

    try:
        file_path = file_obj.name if hasattr(file_obj, "name") else file_obj

        async with httpx.AsyncClient(timeout=60.0) as client:
            with open(file_path, "rb") as f:
                files = {"file": (os.path.basename(file_path), f, "text/csv")}
                res = await client.post(f"{BACKEND_URL}{endpoint}", files=files)

        res.raise_for_status()
        result = res.json()
        return pd.DataFrame(result.get("data", []))

    except Exception as e:
        return pd.DataFrame({"L·ªói": [str(e)]})


# ========================
# LOGIC DASHBOARD
# ========================

def analyze_advanced_dashboard(file_rating, file_trend, file_email):
    """
    X·ª≠ l√Ω ph√¢n t√≠ch chuy√™n s√¢u: Anomaly Detection, Cross-Analysis & Insight Generation
    """
    if not file_rating or not file_trend:
        return [None] * 9 # Tr·∫£ v·ªÅ r·ªóng n·∫øu thi·∫øu file quan tr·ªçng

    # 1. Load Data
    try:
        df_rating = pd.read_csv(file_rating.name)
        df_trend = pd.read_csv(file_trend.name)
        # Email l√† optional, n·∫øu c√≥ th√¨ ƒë·ªçc
        df_email = pd.read_csv(file_email.name) if file_email else pd.DataFrame()
        
        # Pre-processing
        if 'date' in df_trend.columns:
            df_trend['date'] = pd.to_datetime(df_trend['date'], errors='coerce')
        
        # Chu·∫©n h√≥a t√™n c·ªôt (tr√°nh l·ªói case sensitive)
        df_rating.columns = [c.lower() for c in df_rating.columns]
        df_trend.columns = [c.lower() for c in df_trend.columns]

    except Exception as e:
        return [f"L·ªói ƒë·ªçc file: {str(e)}"] + [None]*8

    # ==========================
    # A. TREND ANALYSIS (KPIs & Anomaly)
    # ==========================
    
    # 1. Volume theo ng√†y
    daily_vol = df_trend.groupby('date')['total_volume'].sum().reset_index()
    
    # 2. Anomaly Detection (Ph√°t hi·ªán Spike)
    # Logic: Ng√†y n√†o volume > Mean + 1.5 * StdDev l√† b·∫•t th∆∞·ªùng
    vol_mean = daily_vol['total_volume'].mean()
    vol_std = daily_vol['total_volume'].std()
    threshold = vol_mean + 1.5 * vol_std
    
    df_spikes = daily_vol[daily_vol['total_volume'] > threshold].copy()
    df_spikes['note'] = 'üî• High Volume Spike'
    df_spikes = df_spikes.sort_values('total_volume', ascending=False)

    # 3. Top Negative Topics
    # T√≠nh negative rate: sum(Negative) / sum(Total)
    topic_stats = df_trend.groupby('topic')[['negative', 'total_volume']].sum().reset_index()
    topic_stats['neg_rate'] = (topic_stats['negative'] / topic_stats['total_volume']) * 100
    top_neg_topics = topic_stats.sort_values('neg_rate', ascending=False).head(5)

    # ==========================
    # B. RATING ANALYSIS
    # ==========================
    
    # 1. Avg Rating per Product
    prod_stats = df_rating.groupby('product_id')['predicted_rating'].agg(['mean', 'count']).reset_index()
    prod_stats.columns = ['product_id', 'avg_rating', 'review_count']
    
    # 2. Risky Products (Rating < 3.5)
    risky_products = prod_stats[prod_stats['avg_rating'] < 3.5].sort_values('avg_rating')
    
    # 3. User Variance (Optional): ƒê·ªô l·ªách chu·∫©n rating c·ªßa t·ª´ng user
    # (ƒê·ªÉ xem user n√†o kh√≥ t√≠nh hay d·ªÖ t√≠nh)
    if 'user_id' in df_rating.columns:
        user_var = df_rating.groupby('user_id')['predicted_rating'].std().mean()
    else:
        user_var = 0

    # ==========================
    # C. CROSS-INSIGHTS (T·ªîNG H·ª¢P)
    # ==========================
    
    insights = []
    insights.append("=== B√ÅO C√ÅO PH√ÇN T√çCH T·ªîNG H·ª¢P ===")
    
    # Insight 1: T√¨nh h√¨nh chung
    avg_r = df_rating['predicted_rating'].mean()
    avg_n_rate = (df_trend['negative'].sum() / df_trend['total_volume'].sum()) * 100
    insights.append(f"1. T·ªïng quan:\n   - Rating trung b√¨nh to√†n s√†n: {avg_r:.2f}/5.0\n   - T·ª∑ l·ªá ti√™u c·ª±c tr√™n mxh/trend: {avg_n_rate:.1f}%")
    
    # Insight 2: M√¢u thu·∫´n (Polarization)
    if avg_r > 4.0 and avg_n_rate > 30:
        insights.append("‚ö†Ô∏è C·∫¢NH B√ÅO: S·∫£n ph·∫©m c√≥ Rating cao nh∆∞ng Th·∫£o lu·∫≠n ti√™u c·ª±c nhi·ªÅu -> C√≥ th·ªÉ l√† 'Seeding' ·∫£o ho·∫∑c S·∫£n ph·∫©m g√¢y tranh c√£i.")
    elif avg_r < 3.0 and avg_n_rate > 50:
        insights.append("üö® KH·ª¶NG HO·∫¢NG: Rating th·∫•p v√† D∆∞ lu·∫≠n r·∫•t ti√™u c·ª±c -> C·∫ßn d·ª´ng b√°n ho·∫∑c c·∫£i t·ªï s·∫£n ph·∫©m ngay.")
    else:
        insights.append("‚úÖ ·ªîn ƒë·ªãnh: Ch·ªâ s·ªë Rating v√† Sentiment t∆∞∆°ng ƒë·ªìng.")

    # Insight 3: Spike Analysis
    if not df_spikes.empty:
        spike_dates = df_spikes['date'].dt.strftime('%Y-%m-%d').tolist()
        insights.append(f"üî• Ph√°t hi·ªán b·∫•t th∆∞·ªùng: C√≥ {len(df_spikes)} ng√†y l∆∞·ª£ng th·∫£o lu·∫≠n tƒÉng v·ªçt: {', '.join(spike_dates)}. C·∫ßn ki·ªÉm tra xem l√† kh·ªßng ho·∫£ng hay viral t·ªët.")

    # Insight 4: V·∫•n ƒë·ªÅ c·ª• th·ªÉ
    bad_topic = top_neg_topics.iloc[0]
    insights.append(f"‚ùå Ch·ªß ƒë·ªÅ b·ªã ph√†n n√†n nhi·ªÅu nh·∫•t: '{bad_topic['topic']}' (T·ª∑ l·ªá ti√™u c·ª±c: {bad_topic['neg_rate']:.1f}%).")

    # ==========================
    # D. VISUALIZATION
    # ==========================
    
    # Plot 1: Volume & Spikes (Line Chart)
    fig_trend = plt.figure(figsize=(10, 5))
    sns.lineplot(data=daily_vol, x='date', y='total_volume', marker='o', label='Daily Volume')
    # V·∫Ω ƒëi·ªÉm spike
    if not df_spikes.empty:
        plt.scatter(df_spikes['date'], df_spikes['total_volume'], color='red', s=100, zorder=5, label='Anomaly (Spike)')
    plt.title('Xu h∆∞·ªõng th·∫£o lu·∫≠n & ƒêi·ªÉm b·∫•t th∆∞·ªùng')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()

    # Plot 2: Rating Distribution
    fig_rating = plt.figure(figsize=(6, 5))
    sns.histplot(df_rating['predicted_rating'], bins=20, kde=True, color='green')
    plt.axvline(3.5, color='red', linestyle='--', label='Ng∆∞·ª°ng r·ªßi ro (3.5)')
    plt.title('Ph√¢n b·ªë ƒëi·ªÉm ƒë√°nh gi√°')
    plt.legend()
    plt.tight_layout()

    # Plot 3: Negative Rate by Topic (Bar Chart)
    fig_topic = plt.figure(figsize=(8, 5))
    sns.barplot(data=top_neg_topics, x='neg_rate', y='topic', palette='Reds_r')
    plt.title('Top Ch·ªß ƒë·ªÅ c√≥ t·ª∑ l·ªá ti√™u c·ª±c cao nh·∫•t')
    plt.xlabel('T·ª∑ l·ªá ti√™u c·ª±c (%)')
    plt.tight_layout()

    # ==========================
    # E. EXPORT FILES
    # ==========================
    
    # 1. File Insights Text
    txt_path = os.path.join(GRADIO_TEMP_DIR, f"insights_{int(time.time())}.txt")
    with open(txt_path, "w", encoding="utf-8") as f:
        f.write("\n".join(insights))
        f.write("\n\n--- CHI TI·∫æT S·∫¢N PH·∫®M R·ª¶I RO ---\n")
        f.write(risky_products.head(20).to_string())

    # 2. File Summary CSV (G·ªôp Risky Product + Spikes)
    csv_path = os.path.join(GRADIO_TEMP_DIR, f"summary_analysis_{int(time.time())}.csv")
    # Ch√∫ng ta l∆∞u danh s√°ch s·∫£n ph·∫©m r·ªßi ro l√†m ch√≠nh
    risky_products.to_csv(csv_path, index=False, encoding='utf-8-sig')

    return (
        fig_trend, fig_rating, fig_topic,   # 3 Bi·ªÉu ƒë·ªì
        df_spikes[['date', 'total_volume', 'note']], # Dataframe Spike
        risky_products.head(10),            # Dataframe Risky Products
        "\n".join(insights),                # Text Insights hi·ªÉn th·ªã
        txt_path,                           # File txt download
        csv_path                            # File csv download
    )
    
# ========================
# Wrappers & Helpers
# ========================

def run_async(func, *args):
    """Ch·∫°y h√†m async ƒë∆°n l·∫ª"""
    return asyncio.run(func(*args))

def run_batch_and_export(async_func, file_obj):
    """
    M·ªöI: Wrapper x·ª≠ l√Ω Batch + Xu·∫•t file CSV
    1. Ch·∫°y h√†m async ƒë·ªÉ l·∫•y DataFrame
    2. L∆∞u DataFrame ra file CSV t·∫°m
    3. Tr·∫£ v·ªÅ (DataFrame, FilePath)
    """
    # 1. Ch·∫°y logic AI (Async -> Sync)
    df = asyncio.run(async_func(file_obj))
    
    # 2. L∆∞u ra file CSV
    if df is not None and not df.empty:
        # T·∫°o t√™n file d·ª±a tr√™n th·ªùi gian ƒë·ªÉ kh√¥ng b·ªã tr√πng
        filename = f"export_{int(time.time())}.csv"
        filepath = os.path.join(GRADIO_TEMP_DIR, filename)
        
        # L∆∞u file (utf-8-sig ƒë·ªÉ Excel hi·ªÉn th·ªã ƒë√∫ng ti·∫øng Vi·ªát)
        df.to_csv(filepath, index=False, encoding='utf-8-sig')
        return df, filepath
    
    return df, None


# ========================
# Sentiment
# ========================

async def sentiment_single(text):
    if not text.strip(): return "‚ö†Ô∏è Vui l√≤ng nh·∫≠p n·ªôi dung."
    result = await call_api_post("/sentiment/predict", {"text": text})

    if "error" in result:
        return f"‚ùå {result['error']}"

    return f"C·∫£m x√∫c: {result['sentiment'].upper()} (Conf: {result['confidence']:.2%})"


async def sentiment_batch(file):
    if file is None:
        return pd.DataFrame({"L·ªói": ["Ch∆∞a ch·ªçn file"]})
    return await send_csv_batch("/sentiment/predict_batch", file)


# ========================
# Email
# ========================

async def email_single(text):
    if not text.strip(): return "‚ö†Ô∏è N·ªôi dung tr·ªëng."

    result = await call_api_post("/email/predict", {"text": text})

    if "error" in result:
        return f"‚ùå {result['error']}"

    status = "üö® SPAM" if result["is_spam"] else "‚úÖ H·ª£p l·ªá"
    return f"{status} (Conf: {result['confidence']:.2%})"


async def email_batch(file):
    return await send_csv_batch("/email/predict_batch", file)


# ========================
# Topic
# ========================

async def topic_single(text):
    if not text.strip():
        return "‚ö†Ô∏è Nh·∫≠p vƒÉn b·∫£n."

    result = await call_api_post("/topic/predict", {"text": text})

    if "error" in result:
        return f"‚ùå {result['error']}"

    return f"Ch·ªß ƒë·ªÅ: {result['topic']}"


async def topic_batch(file):
    return await send_csv_batch("/topic/predict_batch", file)


# ========================
# RecSys
# ========================

async def recsys_single(uid, pid, desc):
    payload = {"user_id": uid, "product_id": pid, "description": desc or ""}
    result = await call_api_post("/recsys/predict", payload)

    if "error" in result:
        return f"‚ùå {result['error']}"

    note = "(Cold Start)" if result.get("is_cold_start") else ""
    return f"‚≠ê Rating: {result['predicted_rating']}/5 {note}"


async def recsys_batch(file):
    return await send_csv_batch("/recsys/predict_batch", file)


# ========================
# Trend
# ========================

async def trend_analysis(file):
    return await send_csv_batch("/trend/predict", file)


# ========================
# GIAO DI·ªÜN (UPDATED)
# ========================

with gr.Blocks(title="NexusML AI Platform", theme=gr.themes.Soft()) as demo:

    gr.Markdown("# üöÄ NexusML AI Platform")
    gr.Markdown("H·ªá th·ªëng ph√¢n t√≠ch d·ªØ li·ªáu AI ƒëa m√¥ h√¨nh.")

    with gr.Tabs():

        # ==========================
        # TAB 1: SENTIMENT
        # ==========================
        with gr.Tab("1. C·∫£m x√∫c (Sentiment)"):
            gr.Markdown("### üòê Ph√¢n t√≠ch s·∫Øc th√°i vƒÉn b·∫£n (T√≠ch c·ª±c/Ti√™u c·ª±c)")
            
            s_in = gr.Textbox(label="Nh·∫≠p vƒÉn b·∫£n ki·ªÉm tra nhanh")
            s_out = gr.Textbox(label="K·∫øt qu·∫£", interactive=False)
            gr.Button("Ph√¢n t√≠ch ngay").click(
                lambda x: run_async(sentiment_single, x),
                inputs=s_in, outputs=s_out
            )

            gr.Markdown("---")
            gr.Markdown("### üìÇ X·ª≠ l√Ω h√†ng lo·∫°t (Batch Processing)")
            
            # --- H∆Ø·ªöNG D·∫™N SENTIMENT ---
            with gr.Accordion("üìù H∆∞·ªõng d·∫´n ƒë·ªãnh d·∫°ng file CSV (Xem chi ti·∫øt)", open=False):
                gr.Markdown("""
                **Y√™u c·∫ßu file CSV:**
                - Encoding: `UTF-8`
                - C·∫ßn c√≥ **1 trong c√°c c·ªôt sau**: `text`, `comment`, ho·∫∑c `content`.
                
                **V√≠ d·ª• n·ªôi dung file:**
                ```csv
                text,id
                "S·∫£n ph·∫©m d√πng r·∫•t t·ªët",1
                "Giao h√†ng ch·∫≠m qu√°",2
                ```
                """)
            # ---------------------------

            with gr.Row():
                s_file = gr.File(label="T·∫£i l√™n CSV")
                s_download = gr.File(label="T·∫£i xu·ªëng k·∫øt qu·∫£")
            
            s_df = gr.DataFrame(label="Xem tr∆∞·ªõc d·ªØ li·ªáu")
            
            gr.Button("Ch·∫°y Batch & Export").click(
                lambda f: run_batch_and_export(sentiment_batch, f),
                inputs=s_file, outputs=[s_df, s_download]
            )

        # ==========================
        # TAB 2: EMAIL
        # ==========================
        with gr.Tab("2. Email Spam"):
            gr.Markdown("### üìß Ph√°t hi·ªán Email R√°c (Spam/Ham)")
            
            e_in = gr.Textbox(label="N·ªôi dung Email")
            e_out = gr.Textbox(interactive=False, label="K·∫øt qu·∫£")
            gr.Button("Ki·ªÉm tra").click(
                lambda x: run_async(email_single, x),
                inputs=e_in, outputs=e_out
            )

            gr.Markdown("---")
            gr.Markdown("### üìÇ X·ª≠ l√Ω h√†ng lo·∫°t")

            # --- H∆Ø·ªöNG D·∫™N EMAIL ---
            with gr.Accordion("üìù H∆∞·ªõng d·∫´n ƒë·ªãnh d·∫°ng file CSV", open=False):
                gr.Markdown("""
                **Y√™u c·∫ßu file CSV:**
                - Encoding: `UTF-8`
                - C·∫ßn c√≥ **1 trong c√°c c·ªôt sau**: `text`, `content`, `body`, ho·∫∑c `email`.
                
                **V√≠ d·ª• n·ªôi dung file:**
                ```csv
                text,subject
                "Ch√∫c m·ª´ng b·∫°n tr√∫ng th∆∞·ªüng iPhone...", "Qu√† t·∫∑ng"
                "L·ªãch h·ªçp d·ª± √°n v√†o ng√†y mai", "C√¥ng vi·ªác"
                ```
                """)
            # -----------------------

            with gr.Row():
                e_file = gr.File(label="T·∫£i l√™n CSV")
                e_download = gr.File(label="T·∫£i xu·ªëng k·∫øt qu·∫£")
            
            e_df = gr.DataFrame()
            
            gr.Button("Ch·∫°y Batch & Export").click(
                lambda f: run_batch_and_export(email_batch, f),
                inputs=e_file, outputs=[e_df, e_download]
            )

        # ==========================
        # TAB 3: TOPIC
        # ==========================
        with gr.Tab("3. Ch·ªß ƒë·ªÅ (Topic)"):
            gr.Markdown("### üè∑Ô∏è Ph√¢n lo·∫°i ch·ªß ƒë·ªÅ vƒÉn b·∫£n")
            
            t_in = gr.Textbox(label="VƒÉn b·∫£n")
            t_out = gr.Textbox(interactive=False, label="Ch·ªß ƒë·ªÅ d·ª± ƒëo√°n")
            gr.Button("Ph√¢n lo·∫°i").click(
                lambda x: run_async(topic_single, x),
                inputs=t_in, outputs=t_out
            )

            gr.Markdown("---")
            gr.Markdown("### üìÇ X·ª≠ l√Ω h√†ng lo·∫°t")

            # --- H∆Ø·ªöNG D·∫™N TOPIC ---
            with gr.Accordion("üìù H∆∞·ªõng d·∫´n ƒë·ªãnh d·∫°ng file CSV", open=False):
                gr.Markdown("""
                **Y√™u c·∫ßu file CSV:**
                - Encoding: `UTF-8`
                - C·∫ßn c√≥ **1 trong c√°c c·ªôt sau**: `text`, `comment`, ho·∫∑c `content`.
                
                **V√≠ d·ª• n·ªôi dung file:**
                ```csv
                content
                "Th·ªß t∆∞·ªõng ch√≠nh ph·ªß v·ª´a ban h√†nh ch·ªâ th·ªã m·ªõi..."
                "ƒê·ªôi tuy·ªÉn b√≥ng ƒë√° nam gi√†nh huy ch∆∞∆°ng v√†ng..."
                ```
                """)
            # -----------------------

            with gr.Row():
                t_file = gr.File(label="T·∫£i l√™n CSV")
                t_download = gr.File(label="T·∫£i xu·ªëng k·∫øt qu·∫£")
            
            t_df = gr.DataFrame()
            
            gr.Button("Ch·∫°y Batch & Export").click(
                lambda f: run_batch_and_export(topic_batch, f),
                inputs=t_file, outputs=[t_df, t_download]
            )

        # ==========================
        # TAB 4: RECSYS
        # ==========================
        with gr.Tab("4. G·ª£i √Ω (RecSys)"):
            gr.Markdown("### ‚≠ê D·ª± ƒëo√°n ƒë√°nh gi√° ng∆∞·ªùi d√πng (Rating Prediction)")
            
            

            gr.Markdown("---")
            gr.Markdown("### üìÇ X·ª≠ l√Ω h√†ng lo·∫°t")

            # --- H∆Ø·ªöNG D·∫™N RECSYS (QUAN TR·ªåNG) ---
            with gr.Accordion("üìù H∆∞·ªõng d·∫´n ƒë·ªãnh d·∫°ng file CSV (B·∫Øt bu·ªôc ƒë·ªçc)", open=True):
                gr.Markdown("""
                **C·∫•u tr√∫c file CSV b·∫Øt bu·ªôc:**
                H·ªá th·ªëng s·∫Ω t·ª± ƒë·ªông map c√°c t√™n c·ªôt ph·ªï bi·∫øn v·ªÅ chu·∫©n:
                
                | D·ªØ li·ªáu c·∫ßn | T√™n c·ªôt ch·∫•p nh·∫≠n trong CSV | B·∫Øt bu·ªôc? |
                |-------------|-----------------------------|-----------|
                | **Ng∆∞·ªùi d√πng** | `user_id`, `username`, `reviews.username` | ‚úÖ **C√ì** |
                | **S·∫£n ph·∫©m** | `product_id`, `item_id`, `asins` | ‚úÖ **C√ì** |
                | **Ti√™u ƒë·ªÅ** | `title`, `reviews.title` | ‚ùå (T√πy ch·ªçn) |
                | **N·ªôi dung** | `text`, `reviews.text` | ‚ùå (T√πy ch·ªçn) |
                
                **V√≠ d·ª• m·∫´u:**
                ```csv
                username,product_id,title
                "john_doe","IPHONE15","ƒêi·ªán tho·∫°i r·∫•t m∆∞·ª£t"
                "jane_smith","SAMSUNG_S24","Pin h∆°i y·∫øu"
                ```
                """)
            # -------------------------------------

            with gr.Row():
                r_file = gr.File(label="T·∫£i l√™n CSV")
                r_download = gr.File(label="T·∫£i xu·ªëng k·∫øt qu·∫£")
            
            r_df = gr.DataFrame()
            
            gr.Button("Ch·∫°y Batch & Export").click(
                lambda f: run_batch_and_export(recsys_batch, f),
                inputs=r_file, outputs=[r_df, r_download]
            )

        # ==========================
        # TAB 5: TREND
        # ==========================
        with gr.Tab("5. Xu h∆∞·ªõng (Trend)"):
            gr.Markdown("### üìà Ph√¢n t√≠ch xu h∆∞·ªõng")
            
            # --- H∆Ø·ªöNG D·∫™N TREND ---
            with gr.Accordion("üìù H∆∞·ªõng d·∫´n ƒë·ªãnh d·∫°ng file CSV", open=False):
                gr.Markdown("""
                **L∆∞u √Ω:** T√≠nh nƒÉng ƒëang th·ª≠ nghi·ªám.
                **C·∫•u tr√∫c mong ƒë·ª£i:**
                ```csv
                date,reviews.text
                "2024-01-01","B√¨nh lu·∫≠n v·ªÅ s·∫£n ph·∫©m A"
                "2024-01-02","B√¨nh lu·∫≠n v·ªÅ s·∫£n ph·∫©m B"
                ```
                """)
            # -----------------------

            with gr.Row():
                tr_file = gr.File(label="T·∫£i l√™n CSV")
                tr_download = gr.File(label="T·∫£i xu·ªëng k·∫øt qu·∫£")
            
            tr_df = gr.DataFrame()
            
            gr.Button("Ph√¢n t√≠ch & Export").click(
                lambda f: run_batch_and_export(trend_analysis, f),
                inputs=tr_file, outputs=[tr_df, tr_download]
            )

        # ==========================
        # TAB 6: DASHBOARD B√ÅO C√ÅO 
        # ==========================
        with gr.Tab("6. üìä Dashboard B√°o C√°o"):
            gr.Markdown("### T·ªïng h·ª£p d·ªØ li·ªáu t·ª´ 3 ngu·ªìn: RecSys, Trend & Email Classification")
            
            # 1. Khu v·ª±c Upload
            with gr.Accordion("üìù H∆∞·ªõng d·∫´n Upload 3 file d·ªØ li·ªáu ngu·ªìn", open=True):
                gr.Markdown("""
                ƒê·ªÉ t·∫°o Dashboard, vui l√≤ng upload c√°c file k·∫øt qu·∫£ (`export_...csv`) t·ª´ c√°c Tab tr∆∞·ªõc:
                1. **File Rating:** K·∫øt qu·∫£ t·ª´ Tab 4 (RecSys). C·∫ßn c·ªôt: `product_id`, `predicted_rating`.
                2. **File Trend:** K·∫øt qu·∫£ t·ª´ Tab 5 (Trend). C·∫ßn c·ªôt: `date`, `topic`, `Negative`, `Positive`.
                3. **File Email:** K·∫øt qu·∫£ t·ª´ Tab 2 (Email). C·∫ßn c·ªôt: `label`, `is_spam`.
                """)
            
            with gr.Row():
                d_file_rate = gr.File(label="1. File Rating (Required)")
                d_file_trend = gr.File(label="2. File Trend (Required)")
                d_file_email = gr.File(label="3. File Email (Optional)")
            
            btn_analyze = gr.Button("üöÄ Ch·∫°y Ph√¢n T√≠ch & T√¨m Insight", variant="primary")
        
            gr.Markdown("---")
            
            # Khu v·ª±c Insights Text
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### üí° T·ª± ƒë·ªông ph√°t hi·ªán Insights")
                    txt_insights_view = gr.Textbox(label="AI Summary", lines=10, interactive=False)
                
                with gr.Column(scale=1):
                    gr.Markdown("### üì• T·∫£i b√°o c√°o")
                    dl_txt = gr.File(label="B√°o c√°o chi ti·∫øt (.txt)")
                    dl_csv = gr.File(label="D·ªØ li·ªáu t·ªïng h·ª£p (.csv)")

            # Khu v·ª±c Bi·ªÉu ƒë·ªì
            with gr.Tabs():
                with gr.TabItem("üìà Trend & Anomalies"):
                    plot_trend_view = gr.Plot(label="Bi·ªÉu ƒë·ªì xu h∆∞·ªõng")
                    gr.Markdown("#### ‚ö†Ô∏è C√°c ng√†y c√≥ l∆∞·ª£ng th·∫£o lu·∫≠n tƒÉng ƒë·ªôt bi·∫øn (Spikes)")
                    df_spike_view = gr.DataFrame(label="Danh s√°ch ng√†y b·∫•t th∆∞·ªùng")
                
                with gr.TabItem("‚≠ê Rating Performance"):
                    with gr.Row():
                        plot_rating_view = gr.Plot(label="Ph√¢n b·ªë ƒëi·ªÉm")
                        plot_topic_view = gr.Plot(label="V·∫•n ƒë·ªÅ theo Topic")
                    
                    gr.Markdown("#### üö® Danh s√°ch s·∫£n ph·∫©m r·ªßi ro (Rating < 3.5)")
                    df_risky_view = gr.DataFrame(label="C·∫ßn ki·ªÉm tra g·∫•p")

            # Wiring
            btn_analyze.click(
                fn=analyze_advanced_dashboard,
                inputs=[d_file_rate, d_file_trend, d_file_email],
                outputs=[
                    plot_trend_view, plot_rating_view, plot_topic_view, # Plots
                    df_spike_view, df_risky_view,                       # Dataframes
                    txt_insights_view,                                  # Text Insight
                    dl_txt, dl_csv                                      # Download files
                ]
            )

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860, share=True)